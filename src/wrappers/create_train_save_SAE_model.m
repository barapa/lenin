%function [ filename_of_model ] = create_train_save_SAE_model(...
%    run,...
%    window_size,...
%    window_overlap,...
%    preprocessing_epsilon,...
%    preprocessing_k,...
%    sae_train_percentage,...
%    sae_layer_sizes,...
%    sae_activation_function,...
%    sae_num_epochs,...
%    sae_song_batch_size,...
%    sae_mini_batch_size,...
%    sae_learning_rate,...
%    sae_input_zero_masked_fraction,...
%    nn_input_zero_masked_fraction,...
%    nn_train_percentage,...
%    nn_num_epochs,...
%    nn_batch_size,...
%    nn_learning_rate,...
%    nn_activation_function,...
%    nn_momentum,...
%    nn_plot,...
%    nn_output,...
%    nn_scaling_learning_rate,...
%    nn_weight_penalty_L2,...
%    nn_non_sparsity_penalty,...
%    nn_sparsity_target,...
%    nn_dropout_fraction,...
%    nn_song_batch_size)

% NOTE: the descriptions below are not entirely accurate with the actual params
%       list. Base it on the names of the params list above. 

% run :           an integer between 1 and 10 that indicates which
%                 'run' to use. For example 3 will train assuming
%                 the test data from the 3rd run.
% window_size:    a scalar power to 2 specifying the window size to use for
%                 for the stft.
%                 E.g., 1024
% window_overlap: a scalar representing the overlap between windows for the
%                 stft.
%                 E.g., 512 
% nfft: 
% preprocessing_epsilon:
%                 whitening parameter. .00001 is good.
% preprocessing_k:
%                 used for pca whitening. This will be the number of dimensions
%                 of the visible vectors                
% sae_train_percentage: 
%                 Integer indicating the amount of training data to
%                 use to train the unsupervised sae with. Must be 30, 60, or 90.
%                 Songs will be selected from the run specification.
% sae_layer_sizes: 
%                 a 1xL vector of integers indicating the size of each hidden layer.
%                 Does not include the final output layer of the FFNN or the
%                 visible input layer.
%                 Ex. [1000, 500, 100]
% sae_num_epochs: integer for number of training epochs for sae
% sae_song_batch_size:
%                 integer for number of songs in a batch
% sae_mini_batch_size:
%                 integer specifying the traditional training batch size
%                 as described in the literature
% sae_input_zero_masked_fraction: set to .5 or so. This is the noise added
%                 to the denoising autoencoder.
% nn_train_percentage:
%                 30, 60, or 90. For now, 90 will be too big so do 30.
% nn_num_epochs:  integer number of training epochs
% nn_batch_size:  integer batch size of each training step
% nn_learning_rate: 
%                 learning rate. Typically needs to be lower when using 'sigm'
%                 activation function and non-normalized inputs
% nn_activation_function:
%                 activation functions of hidden layers. 'sigm' for sigmoid
%                 or 'tanh_opt' for optimal tanh.
% nn_momentum:    momentum
% nn_plot:        set to 1 if you want to plot the output of the training,
%                 0 otherwise
% nn_output:      final output unit. 'sigm', 'softmax' or 'linear'.
% nn_scaling_learning_rate
%               : Scaling factor for the learning rate (each epoch).
% nn_weight_penalty_L2
%               : L2 regularization on weights. 0 for no regularization.
% nn_non_sparsity_penalty
%               : Non-sparsity penalty. Penalty punishing non-sparsity.
% nn_sparsity_target
%               : Sparsity target (generally set to 0.05). Does nothing if
%                 if non_sparsity_penalty is set to zero.
% nn_dropout_fraction
%               : Randomly omits this fraction of feature detectors on each
%                 training case. Supposedly leads to better results and less
%                 over fitting. See Hinton paper:
%                 http://arxiv.org/abs/1207.0580
%
% filename_of_model
%               : returned value is the filename of the saved model, without the
%                 path. Example: 'rbm_dbn_20130417T123919.mat'
% nn_song_batch_size
%               : song batch size for training nn, for memory conservation

function [ filename_of_model ] = create_train_save_SAE_model(...
    run,...
    window_size,...
    window_overlap,...
    nfft,...
    preprocessing_epsilon,...
    preprocessing_k,...
    sae_train_percentage,...
    sae_layer_sizes,...
    sae_activation_function,...
    sae_num_epochs,...
    sae_song_batch_size,...
    sae_mini_batch_size,...
    sae_learning_rate,...
    sae_input_zero_masked_fraction,...
    nn_input_zero_masked_fraction,...
    nn_train_percentage,...
    nn_num_epochs,...
    nn_batch_size,...
    nn_learning_rate,...
    nn_activation_function,...
    nn_momentum,...
    nn_plot,...
    nn_output,...
    nn_scaling_learning_rate,...
    nn_weight_penalty_L2,...
    nn_non_sparsity_penalty,...
    nn_sparsity_target,...
    nn_dropout_fraction,...
    nn_song_batch_size)

% Ensure stft data exists for given params
stft_beatles_songs(window_size, window_overlap, nfft);

% get song names for training and testing
[sae_train_song_names test_song_names, ~] = load_run_data(run,...
  sae_train_percentage);
[nn_train_song_names ~ ~] = load_run_data(run, nn_train_percentage);

% get full paths to the songs for this run and the stft params
sae_train_file_names = get_song_data_full_paths(sae_train_song_names,...
  window_size, window_overlap, nfft);
nn_train_file_names = get_song_data_full_paths(nn_train_song_names,...
  window_size, window_overlap, nfft);
test_file_names = get_song_data_full_paths(test_song_names,...
  window_size, window_overlap, nfft);

% set up whitening parameters (and stft params for the sake of saving)
preprocessing_params = create_dbn_pre_processing_params(...
    preprocessing_epsilon, preprocessing_k, window_size, window_overlap, nfft);

% ---- SAE SETUP AND TRAINING ----
% set up dbn network topology
sae_network_params = create_sae_network_params(sae_layer_sizes,...
  sae_activation_function );

% set up dbn training params
sae_training_params = create_sae_training_params(sae_num_epochs,...
  sae_song_batch_size, sae_mini_batch_size, sae_learning_rate,...
  sae_input_zero_masked_fraction);

% train sae, saving preprocessing params for later use
[sae, sae_sizes, preprocessing_params] = pre_train_sae(sae_train_file_names,...
  sae_network_params, sae_training_params, preprocessing_params);

% ---- FFNN SETUP AND TRAINING ----

% load data for training and testing. We will use same data for testing as
% for validation for now, since the only thing we do with the validation
% set is plot its results.

% setup ffnn parameters
nn_training_params = create_nn_training_params(...
    nn_num_epochs, nn_batch_size, nn_learning_rate, nn_activation_function,...
    nn_momentum, nn_plot, nn_output, nn_scaling_learning_rate,...
    nn_weight_penalty_L2, nn_non_sparsity_penalty, nn_sparsity_target,...
    nn_input_zero_masked_fraction, nn_dropout_fraction, nn_song_batch_size)

% train
nn = train_sae_nn_song_batches( sae, sae_sizes, nn_training_params,...
    nn_train_file_names, preprocessing_params, test_file_names);
    
% calculate and print error
fprintf('%s\n', 'Loading testing data');
[test_nn_x, ~, test_nn_y] = load_songs(test_file_names);
fprintf('%s', 'Whitening testing data...');
test_nn_x = whiten_data(test_nn_x, preprocessing_params.X_avg,...
    preprocessing_params.W);
fprintf('done\n');

[error_rate, bad] = nntest(nn, test_nn_x', test_nn_y');
disp(['Error rate: ' num2str(error_rate)]);
    
filename_of_model = save_sae(...
    sae,...
    sae_training_params,...
    sae_network_params,...
    sae_train_file_names,...
    test_file_names,...
    nn_train_file_names,...
    nn,...
    nn_training_params,...
    error_rate,...
    run,...
    nn_train_percentage,...
    sae_train_percentage,...
    'Stacked Autoencoder',...
    preprocessing_params);
end

